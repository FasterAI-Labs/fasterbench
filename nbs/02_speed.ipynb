{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# speed\n",
    "\n",
    "> Speed modules for benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import math, time\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, Sequence, List, Mapping\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.benchmark import Timer\n",
    "\n",
    "\n",
    "from fasterbench.core import _device_ctx, _sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class SpeedMetrics:\n",
    "    p50_ms: float\n",
    "    p90_ms: float\n",
    "    p99_ms: float\n",
    "    mean_ms: float\n",
    "    std_ms: float\n",
    "    throughput_s: float\n",
    "\n",
    "    def as_dict(self):\n",
    "        return asdict(self)\n",
    "\n",
    "\n",
    "#| export\n",
    "def _stats(lat_ms: np.ndarray, batch: int) -> Mapping[str, float]:\n",
    "    return {\n",
    "        \"p50_ms\": float(np.percentile(lat_ms, 50)),\n",
    "        \"p90_ms\": float(np.percentile(lat_ms, 90)),\n",
    "        \"p99_ms\": float(np.percentile(lat_ms, 99)),\n",
    "        \"mean_ms\": float(lat_ms.mean()),\n",
    "        \"std_ms\": float(lat_ms.std(ddof=1)),\n",
    "        \"throughput_s\": batch * 1000.0 / float(lat_ms.mean()),\n",
    "    }\n",
    "\n",
    "#| export\n",
    "def _forward_latencies(\n",
    "    model: nn.Module,\n",
    "    sample: torch.Tensor,\n",
    "    *,\n",
    "    device: str | torch.device = \"cpu\",\n",
    "    warmup: int = 20,\n",
    "    steps: int = 100,\n",
    "    use_torch_timer: bool | None = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Collect `steps` forward-pass latencies (ms).  Uses the low-overhead\n",
    "    ``torch.utils.benchmark.Timer`` on CPU and ``torch.cuda.Event`` on GPU.\n",
    "    \"\"\"\n",
    "    if use_torch_timer is None:\n",
    "        use_torch_timer = torch.device(device).type == \"cpu\"\n",
    "\n",
    "    with _device_ctx(device) as dev:\n",
    "        model.eval().to(dev)\n",
    "        sample = sample.to(dev, non_blocking=True)\n",
    "\n",
    "        # ─ warm-up\n",
    "        for _ in range(warmup):\n",
    "            model(sample)\n",
    "        _sync(dev)\n",
    "\n",
    "        lat: List[float] = []\n",
    "        if use_torch_timer and dev.type == \"cpu\":\n",
    "            t = Timer(stmt=\"model(x)\", globals={\"model\": model, \"x\": sample})\n",
    "            m = t.blocked_autorange(min_run_time=0.3)\n",
    "            per_iter = (np.asarray(m.raw_times) / m.number_per_run) * 1e3  # ns→ms\n",
    "            if per_iter.size < steps:\n",
    "                per_iter = np.resize(per_iter, steps)   # pad so len==steps\n",
    "            lat = per_iter.tolist()\n",
    "        elif dev.type == \"cuda\":\n",
    "            start_evt, end_evt = torch.cuda.Event(True), torch.cuda.Event(True)\n",
    "            for _ in range(steps):\n",
    "                start_evt.record()\n",
    "                model(sample)\n",
    "                end_evt.record()\n",
    "                _sync(dev)\n",
    "                lat.append(start_evt.elapsed_time(end_evt))  # ms\n",
    "        else:  # plain time.perf_counter fallback\n",
    "            for _ in range(steps):\n",
    "                t0 = time.perf_counter()\n",
    "                model(sample)\n",
    "                lat.append((time.perf_counter() - t0) * 1e3)\n",
    "        return np.asarray(lat, dtype=np.float32)\n",
    "\n",
    "#| export\n",
    "@torch.inference_mode()\n",
    "def compute_speed(model: nn.Module,\n",
    "                  sample: torch.Tensor,\n",
    "                  *,\n",
    "                  device: str | torch.device = \"cpu\",\n",
    "                  warmup: int = 20,\n",
    "                  steps: int = 100) -> SpeedMetrics:\n",
    "    \"\"\"\n",
    "    Measure latency/throughput on *one* device.  Returns a `SpeedMetrics`.\n",
    "    \"\"\"\n",
    "    with _device_ctx(device) as dev:\n",
    "        lat = _forward_latencies(\n",
    "            model, sample, device=dev, warmup=warmup, steps=steps\n",
    "        )\n",
    "        s = _stats(lat, sample.size(0))\n",
    "        return SpeedMetrics(**s)\n",
    "\n",
    "#| export\n",
    "def compute_speed_multi(model: nn.Module,\n",
    "                        sample: torch.Tensor,\n",
    "                        *,\n",
    "                        devices: Sequence[str | torch.device] | None = None,\n",
    "                        **kwargs) -> Dict[str, SpeedMetrics]:\n",
    "    \"\"\"\n",
    "    Convenience wrapper: returns ``dict[str, SpeedMetrics]`` keyed by device.\n",
    "    Defaults to both CPU *and* CUDA if a GPU is available.\n",
    "    \"\"\"\n",
    "    if devices is None:\n",
    "        devices = [\"cpu\"]\n",
    "        if torch.cuda.is_available():\n",
    "            devices.append(\"cuda\")\n",
    "\n",
    "    out: Dict[str, SpeedMetrics] = {}\n",
    "    for d in devices:\n",
    "        try:\n",
    "            out[str(d)] = compute_speed(model, sample, device=d, **kwargs)\n",
    "        except RuntimeError:\n",
    "            out[str(d)] = SpeedMetrics(*[math.nan] * 6)\n",
    "    return out\n",
    "\n",
    "#| export\n",
    "def sweep_threads(model: nn.Module,\n",
    "                  sample: torch.Tensor,\n",
    "                  thread_counts: Sequence[int] = (1, 2, 4, 8),\n",
    "                  *,\n",
    "                  warmup: int = 20,\n",
    "                  steps: int = 100) -> \"torch.Tensor\":\n",
    "    \"\"\"\n",
    "    Return a *pandas-compatible* list-of-dicts; each row contains latency\n",
    "    stats for a different ``torch.set_num_threads(n)``.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for n in thread_counts:\n",
    "        torch.set_num_threads(n)\n",
    "        lat = _forward_latencies(\n",
    "            model, sample,\n",
    "            device=\"cpu\",\n",
    "            warmup=warmup,\n",
    "            steps=steps,\n",
    "            use_torch_timer=True,\n",
    "        )\n",
    "        row = {\"threads\": n, **_stats(lat, sample.size(0))}\n",
    "        rows.append(row)\n",
    "    return rows   # user can `pd.DataFrame(rows)` if they wish\n",
    "\n",
    "#| export\n",
    "def sweep_latency(model: nn.Module,\n",
    "                  shapes: Sequence[Sequence[int]],\n",
    "                  *,\n",
    "                  device: str | torch.device = \"cuda\",\n",
    "                  warmup: int = 20,\n",
    "                  steps: int = 100) -> \"torch.Tensor\":\n",
    "    \"\"\"\n",
    "    Sweep different *input shapes* on the same device.\n",
    "    Useful for CNNs/ViTs where latency scales with resolution.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for shape in shapes:\n",
    "        dummy = torch.empty(*shape)\n",
    "        lat = _forward_latencies(\n",
    "            model, dummy,\n",
    "            device=device,\n",
    "            warmup=warmup,\n",
    "            steps=steps,\n",
    "        )\n",
    "        row = {\"shape\": \"×\".join(map(str, shape)), **_stats(lat, shape[0])}\n",
    "        rows.append(row)\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
