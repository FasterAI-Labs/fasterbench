{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ad1c615-6008-49ef-aac2-bf06bec096a7",
   "metadata": {},
   "source": [
    "# plot\n",
    "\n",
    "> A module to plot the results of the benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0968fc9f-174e-4674-949d-2d987e9bd83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c86ca8f-38d2-44d0-8204-3c977f1b1c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import plotly.graph_objects as go\n",
    "from fasterbench.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58201045-9b23-4db4-a003-da7a4388fa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_radar_plot(benchmark_results):\n",
    "    # Define metrics with icons, hover text format, and units\n",
    "    metrics = {\n",
    "        'üíæ': {  # Storage icon\n",
    "            'value': benchmark_results['disk_size'] / 1e6,\n",
    "            'hover_format': 'Model Size: {:.2f} MB',\n",
    "            'unit': 'MB'\n",
    "        },\n",
    "        'üßÆ': {  # Calculator icon for parameters\n",
    "            'value': parse_metric_value(benchmark_results['num_parameters']),\n",
    "            'hover_format': 'Parameters: {:.2f}M',\n",
    "            'unit': 'M'\n",
    "        },\n",
    "        '‚è±Ô∏è': {  # Clock icon for latency\n",
    "            'value': benchmark_results['cpu_latency'],\n",
    "            'hover_format': 'Latency: {:.2f} ms',\n",
    "            'unit': 'ms'\n",
    "        },\n",
    "        '‚ö°': {  # Lightning bolt for MACs\n",
    "            'value': parse_metric_value(benchmark_results['macs']),\n",
    "            'hover_format': 'MACs: {:.2f}G',\n",
    "            'unit': 'G'\n",
    "        },\n",
    "        'üîã': {  # Battery icon for energy\n",
    "            'value': benchmark_results['avg_energy'] * 1e6,\n",
    "            'hover_format': 'Energy: {:.3f} mWh',\n",
    "            'unit': 'mWh'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Find min and max values for each metric\n",
    "    reference_values = {\n",
    "        'üíæ': {'min': 0, 'max': max(metrics['üíæ']['value'], 500)},    # Model size (MB)\n",
    "        'üßÆ': {'min': 0, 'max': max(metrics['üßÆ']['value'], 50)},     # Parameters (M)\n",
    "        '‚è±Ô∏è': {'min': 0, 'max': max(metrics['‚è±Ô∏è']['value'], 200)},    # Latency (ms)\n",
    "        '‚ö°': {'min': 0, 'max': max(metrics['‚ö°']['value'], 5000)},      # MACs (G)\n",
    "        'üîã': {'min': 0, 'max': max(metrics['üîã']['value'], 5)}       # Energy (mWh)\n",
    "    }\n",
    "    \n",
    "    # Normalize values and create hover text\n",
    "    normalized_values = []\n",
    "    hover_texts = []\n",
    "    labels = []\n",
    "    \n",
    "    for icon, metric in metrics.items():\n",
    "        # Min-max normalization\n",
    "        normalized_value = (metric['value'] - reference_values[icon]['min']) / \\\n",
    "                         (reference_values[icon]['max'] - reference_values[icon]['min'])\n",
    "        normalized_values.append(normalized_value)\n",
    "        \n",
    "        # Create hover text with actual value\n",
    "        hover_texts.append(metric['hover_format'].format(metric['value']))\n",
    "        labels.append(icon)\n",
    "    \n",
    "    # Add first values again to close the polygon\n",
    "    normalized_values.append(normalized_values[0])\n",
    "    hover_texts.append(hover_texts[0])\n",
    "    labels.append(labels[0])\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "        r=normalized_values,\n",
    "        theta=labels,\n",
    "        fill='toself',\n",
    "        name='Model Metrics',\n",
    "        hovertext=hover_texts,\n",
    "        hoverinfo='text',\n",
    "        line=dict(color='#FF8C00'),  # Bright orange color\n",
    "        fillcolor='rgba(255, 140, 0, 0.3)'  # Semi-transparent orange\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        polar=dict(\n",
    "            radialaxis=dict(\n",
    "                visible=True,\n",
    "                range=[0, 1],\n",
    "                showticklabels=False,  # Hide radial axis labels\n",
    "                gridcolor='rgba(128, 128, 128, 0.5)',  # Semi-transparent grey grid lines\n",
    "                linecolor='rgba(128, 128, 128, 0.5)'   # Semi-transparent grey axis lines\n",
    "            ),\n",
    "            angularaxis=dict(\n",
    "                tickfont=dict(size=24),  # Icon labels\n",
    "                gridcolor='rgba(128, 128, 128, 0.5)'  # Semi-transparent grey grid lines\n",
    "            ),\n",
    "            bgcolor='rgba(0,0,0,0)'  # Transparent background\n",
    "        ),\n",
    "        showlegend=False,\n",
    "\n",
    "        margin=dict(t=100, b=100, l=100, r=100),\n",
    "        paper_bgcolor='rgba(0,0,0,0)',  # Transparent background\n",
    "        plot_bgcolor='rgba(0,0,0,0)'    # Transparent background\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aca5c3c-7208-4d63-855e-f488059c11ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_tradeoff_plot(model_metrics_list, model_names, x_metric, y_metric, \n",
    "                         x_label=None, y_label=None, show_pareto=True,\n",
    "                         higher_is_better_x=False, higher_is_better_y=True,\n",
    "                         title=None, width=800, height=600):\n",
    "\n",
    "    import plotly.graph_objects as go\n",
    "    import numpy as np\n",
    "    from plotly.subplots import make_subplots\n",
    "    \n",
    "    # Extract data points\n",
    "    x_values = []\n",
    "    y_values = []\n",
    "    hover_texts = []\n",
    "    \n",
    "    for i, metrics in enumerate(model_metrics_list):\n",
    "        try:\n",
    "            # Handle string metric values with units\n",
    "            x_val = parse_metric_value(metrics.get(x_metric, 0))\n",
    "            y_val = parse_metric_value(metrics.get(y_metric, 0))\n",
    "            \n",
    "            x_values.append(x_val)\n",
    "            y_values.append(y_val)\n",
    "            \n",
    "            # Create hover text with model name and both metrics\n",
    "            hover_text = f\"{model_names[i]}<br>{x_metric}: {x_val}<br>{y_metric}: {y_val}\"\n",
    "            hover_texts.append(hover_text)\n",
    "        except (ValueError, KeyError):\n",
    "            print(f\"Warning: Could not extract {x_metric} or {y_metric} for model {model_names[i]}\")\n",
    "    \n",
    "    # Create figure\n",
    "    fig = make_subplots()\n",
    "    \n",
    "    # Add scatter plot for models\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=x_values,\n",
    "        y=y_values,\n",
    "        mode='markers+text',\n",
    "        marker=dict(\n",
    "            size=12,\n",
    "            color='rgba(31, 119, 180, 0.8)',\n",
    "            line=dict(width=1, color='black')\n",
    "        ),\n",
    "        text=model_names,\n",
    "        textposition=\"top center\",\n",
    "        hovertext=hover_texts,\n",
    "        hoverinfo='text',\n",
    "        name='Models'\n",
    "    ))\n",
    "    \n",
    "    # Compute and add Pareto frontier if requested\n",
    "    if show_pareto and len(x_values) > 1:\n",
    "        # Convert \"higher is better\" to \"lower is better\" for algorithm\n",
    "        x_arr = np.array([-x if higher_is_better_x else x for x in x_values])\n",
    "        y_arr = np.array([-y if higher_is_better_y else y for y in y_values])\n",
    "        \n",
    "        # Find Pareto-optimal points\n",
    "        pareto_points = []\n",
    "        pareto_indices = []\n",
    "        \n",
    "        for i in range(len(x_arr)):\n",
    "            dominated = False\n",
    "            for j in range(len(x_arr)):\n",
    "                if i != j:\n",
    "                    if x_arr[j] <= x_arr[i] and y_arr[j] <= y_arr[i] and (x_arr[j] < x_arr[i] or y_arr[j] < y_arr[i]):\n",
    "                        dominated = True\n",
    "                        break\n",
    "            if not dominated:\n",
    "                pareto_points.append((x_values[i], y_values[i]))\n",
    "                pareto_indices.append(i)\n",
    "        \n",
    "        # Sort points for line drawing\n",
    "        if higher_is_better_x:\n",
    "            pareto_points.sort(key=lambda p: p[0])  # Sort by x ascending\n",
    "        else:\n",
    "            pareto_points.sort(key=lambda p: p[0], reverse=True)  # Sort by x descending\n",
    "        \n",
    "        # Extract sorted x and y values\n",
    "        pareto_x = [p[0] for p in pareto_points]\n",
    "        pareto_y = [p[1] for p in pareto_points]\n",
    "        \n",
    "        # Add line connecting Pareto-optimal points\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=pareto_x,\n",
    "            y=pareto_y,\n",
    "            mode='lines',\n",
    "            line=dict(color='rgba(255, 0, 0, 0.7)', width=2, dash='dash'),\n",
    "            name='Pareto Frontier'\n",
    "        ))\n",
    "        \n",
    "        # Highlight Pareto-optimal points\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[x_values[i] for i in pareto_indices],\n",
    "            y=[y_values[i] for i in pareto_indices],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=16,\n",
    "                color='rgba(255, 0, 0, 0.7)',\n",
    "                symbol='circle-open',\n",
    "                line=dict(width=3, color='red')\n",
    "            ),\n",
    "            hoverinfo='skip',\n",
    "            showlegend=False\n",
    "        ))\n",
    "    \n",
    "    # Set axis labels\n",
    "    fig.update_layout(\n",
    "        xaxis_title=x_label if x_label else x_metric,\n",
    "        yaxis_title=y_label if y_label else y_metric,\n",
    "        title=title if title else f\"{y_metric} vs {x_metric} Trade-off\",\n",
    "        width=width,\n",
    "        height=height,\n",
    "        template=\"plotly_white\",\n",
    "        legend=dict(\n",
    "            yanchor=\"top\",\n",
    "            y=0.99,\n",
    "            xanchor=\"right\",\n",
    "            x=0.99\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Add annotations for ideal direction\n",
    "    annotations = []\n",
    "    \n",
    "    # X-axis direction arrow\n",
    "    arrow_x = {\"x\": 0.95, \"y\": 0.02, \"text\": \"‚Üí Better\", \"showarrow\": False, \n",
    "               \"xref\": \"paper\", \"yref\": \"paper\", \"font\": {\"size\": 12}}\n",
    "    if not higher_is_better_x:\n",
    "        arrow_x[\"x\"] = 0.05\n",
    "        arrow_x[\"text\"] = \"‚Üê Better\"\n",
    "    annotations.append(arrow_x)\n",
    "    \n",
    "    # Y-axis direction arrow\n",
    "    arrow_y = {\"x\": 0.01, \"y\": 0.95, \"text\": \"‚Üí Better\", \"showarrow\": False, \n",
    "               \"xref\": \"paper\", \"yref\": \"paper\", \"textangle\": -90, \"font\": {\"size\": 12}}\n",
    "    if not higher_is_better_y:\n",
    "        arrow_y[\"y\"] = 0.05\n",
    "        arrow_y[\"text\"] = \"‚Üì Better\"\n",
    "    annotations.append(arrow_y)\n",
    "    \n",
    "    fig.update_layout(annotations=annotations)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65567e8-ece6-4026-89be-904c414fefa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def plot_tradeoffs(model_list, dls, model_names=None):\n",
    "    \"\"\"Benchmark multiple models and create comparative plots\"\"\"\n",
    "    if model_names is None:\n",
    "        model_names = [f\"Model {i+1}\" for i in range(len(model_list))]\n",
    "    \n",
    "    # Collect metrics for all models\n",
    "    metrics_list = []\n",
    "    for model in model_list:\n",
    "        dummy_input = torch.Tensor(next(iter(dls.valid))[0][0][None]).to('cpu')\n",
    "        metrics = compute_model_metrics(model, dls, dummy_input)\n",
    "        metrics_list.append(metrics)\n",
    "    \n",
    "    # Create tradeoff plots\n",
    "    acc_lat_plot = create_tradeoff_plot(\n",
    "        metrics_list,\n",
    "        model_names,\n",
    "        x_metric=\"latency\", \n",
    "        y_metric=\"accuracy\",\n",
    "        x_label=\"Latency (ms/sample)\",\n",
    "        y_label=\"Accuracy (%)\",\n",
    "        higher_is_better_x=False,\n",
    "        higher_is_better_y=True,\n",
    "        title=\"Accuracy vs Latency Trade-off\"\n",
    "    )\n",
    "    acc_lat_plot.show()\n",
    "    \n",
    "    size_acc_plot = create_tradeoff_plot(\n",
    "        metrics_list, \n",
    "        model_names,\n",
    "        x_metric=\"params\", \n",
    "        y_metric=\"accuracy\",\n",
    "        x_label=\"Model Size (M params)\",\n",
    "        y_label=\"Accuracy (%)\",\n",
    "        higher_is_better_x=False,  # Smaller models are better\n",
    "        higher_is_better_y=True    # Higher accuracy is better\n",
    "    )\n",
    "    size_acc_plot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
