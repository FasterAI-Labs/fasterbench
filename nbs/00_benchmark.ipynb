{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# benchmark\n",
    "\n",
    "> A module to benchmark Pytorch model according to: size, speed, compute and energy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import time\n",
    "from codecarbon import OfflineEmissionsTracker\n",
    "import numpy as np\n",
    "import os\n",
    "from thop import profile, clever_format\n",
    "from tqdm.notebook import tqdm\n",
    "from prettytable import PrettyTable\n",
    "from torchprofile import profile_macs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_model_size(model, temp_path=\"temp_model.pth\"):\n",
    "    torch.save(model.state_dict(), temp_path)\n",
    "    model_size = os.path.getsize(temp_path)\n",
    "    os.remove(temp_path)\n",
    "    \n",
    "    return model_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_num_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@torch.inference_mode()\n",
    "def evaluate_gpu_speed(model, dummy_input, warmup_rounds=50, test_rounds=100):\n",
    "    device = torch.device(\"cuda\")\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    dummy_input = dummy_input.to(device)\n",
    "    \n",
    "    starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "    latencies = []\n",
    "\n",
    "    # Warm up GPU\n",
    "    for _ in range(warmup_rounds):\n",
    "        _ = model(dummy_input)\n",
    "    \n",
    "    # Measure Latency\n",
    "    for _ in range(test_rounds):\n",
    "        starter.record()\n",
    "        _ = model(dummy_input)\n",
    "        ender.record()\n",
    "        torch.cuda.synchronize()\n",
    "        latencies.append(starter.elapsed_time(ender))  # time in milliseconds\n",
    "    \n",
    "    latencies = np.array(latencies)\n",
    "    mean_latency = np.mean(latencies)\n",
    "    std_latency = np.std(latencies)\n",
    "\n",
    "    # Measure Throughput\n",
    "    throughput = dummy_input.size(0) * 1000 / mean_latency  # Inferences per second\n",
    "\n",
    "    return mean_latency, std_latency, throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@torch.inference_mode()\n",
    "def evaluate_cpu_speed(model, dummy_input, warmup_rounds=50, test_rounds=100):\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    dummy_input = dummy_input.to(device)\n",
    "    \n",
    "    # Warm up CPU\n",
    "    for _ in range(warmup_rounds):\n",
    "        _ = model(dummy_input)\n",
    "    \n",
    "    # Measure Latency\n",
    "    latencies = []\n",
    "    for _ in range(test_rounds):\n",
    "        start_time = time.perf_counter()\n",
    "        _ = model(dummy_input)\n",
    "        end_time = time.perf_counter()\n",
    "        latencies.append(end_time - start_time)\n",
    "    \n",
    "    latencies = np.array(latencies) * 1000  # Convert to milliseconds\n",
    "    mean_latency = np.mean(latencies)\n",
    "    std_latency = np.std(latencies)\n",
    "\n",
    "    # Measure Throughput\n",
    "    throughput = dummy_input.size(0) * 1000 / mean_latency  # Inferences per second\n",
    "\n",
    "    return mean_latency, std_latency, throughput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@torch.inference_mode()\n",
    "def get_model_macs(model, inputs) -> int:\n",
    "    return profile_macs(model, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@torch.inference_mode()\n",
    "def evaluate_gpu_memory_usage(model, dummy_input, warmup_rounds=10, test_rounds=100):\n",
    "    device = torch.device(\"cuda\")\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    dummy_input = dummy_input.to(device)\n",
    "    \n",
    "    # Warm up GPU\n",
    "    for _ in range(warmup_rounds):\n",
    "        _ = model(dummy_input)\n",
    "    \n",
    "    # Measure Memory Usage\n",
    "    memory_usages = []\n",
    "    for _ in range(test_rounds):\n",
    "        torch.cuda.reset_peak_memory_stats(device)\n",
    "        _ = model(dummy_input)\n",
    "        torch.cuda.synchronize()\n",
    "        memory_usages.append(torch.cuda.memory_allocated(device))\n",
    "    \n",
    "    memory_usages = np.array(memory_usages)\n",
    "    average_memory_usage = np.mean(memory_usages)\n",
    "    peak_memory_usage = torch.cuda.max_memory_allocated(device)\n",
    "    \n",
    "    return average_memory_usage, peak_memory_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@torch.inference_mode()\n",
    "def evaluate_emissions(model, dummy_input, warmup_rounds=50, test_rounds=100):\n",
    "    device = torch.device(\"cuda\")\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    dummy_input = dummy_input.to(device)\n",
    "\n",
    "    # Warm up GPU\n",
    "    for _ in range(warmup_rounds):\n",
    "        _ = model(dummy_input)\n",
    "    \n",
    "    # Measure Latency\n",
    "    tracker = OfflineEmissionsTracker(country_iso_code=\"USA\")\n",
    "    tracker.start()\n",
    "    for _ in range(test_rounds):\n",
    "        _ = model(dummy_input)\n",
    "    tracker.stop()\n",
    "    total_emissions = tracker.final_emissions\n",
    "    total_energy_consumed = tracker.final_emissions_data.energy_consumed\n",
    "    \n",
    "    # Calculate average emissions and energy consumption per inference\n",
    "    average_emissions_per_inference = total_emissions / test_rounds\n",
    "    average_energy_per_inference = total_energy_consumed / test_rounds\n",
    "    \n",
    "    return average_emissions_per_inference, average_energy_per_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@torch.inference_mode()\n",
    "def benchmark(model, dummy_input):\n",
    "    # Model Size\n",
    "    disk_size = get_model_size(model)\n",
    "    #num_parameters = get_num_parameters(model)\n",
    "    \n",
    "    # GPU Speed\n",
    "    gpu_latency, gpu_std_latency, gpu_throughput = evaluate_gpu_speed(model, dummy_input)\n",
    "    \n",
    "    # CPU Speed\n",
    "    cpu_latency, cpu_std_latency, cpu_throughput = evaluate_cpu_speed(model, dummy_input)\n",
    "    \n",
    "    # Model MACs\n",
    "    #macs = get_model_macs(model, dummy_input)\n",
    "    macs, params = profile(model, inputs=(dummy_input, ))\n",
    "    macs, num_parameters = clever_format([macs, params], \"%.3f\")\n",
    "    \n",
    "    # GPU Memory Usage\n",
    "    avg_gpu_memory, peak_gpu_memory = evaluate_gpu_memory_usage(model, dummy_input)\n",
    "    \n",
    "    # Emissions\n",
    "    avg_emissions, avg_energy = evaluate_emissions(model, dummy_input)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters} parameters\")\n",
    "    print(f\"GPU Latency: {gpu_latency:.3f} ms (± {gpu_std_latency:.3f} ms)\")\n",
    "    print(f\"GPU Throughput: {gpu_throughput:.2f} inferences/sec\")\n",
    "    print(f\"CPU Latency: {cpu_latency:.3f} ms (± {cpu_std_latency:.3f} ms)\")\n",
    "    print(f\"CPU Throughput: {cpu_throughput:.2f} inferences/sec\")\n",
    "    print(f\"Model MACs: {macs}\")\n",
    "    print(f\"Average GPU Memory Usage: {avg_gpu_memory / 1e6:.2f} MB\")\n",
    "    print(f\"Peak GPU Memory Usage: {peak_gpu_memory / 1e6:.2f} MB\")\n",
    "    print(f\"Average Carbon Emissions per Inference: {avg_emissions*1e3:.6f} gCO2e\")\n",
    "    print(f\"Average Energy Consumption per Inference: {avg_energy*1e3:.6f} Wh\")\n",
    "\n",
    "    return {\n",
    "\n",
    "        'disk_size': disk_size,\n",
    "        'num_parameters': num_parameters, \n",
    "        'gpu_latency': gpu_latency, \n",
    "        'gpu_throughput': gpu_throughput,\n",
    "        'cpu_latency': cpu_latency,\n",
    "        'cpu_throughput': cpu_throughput,\n",
    "        'macs': macs, \n",
    "        'avg_gpu_memory': avg_gpu_memory, \n",
    "        'peak_gpu_memory': peak_gpu_memory,\n",
    "        'avg_emissions': avg_emissions, \n",
    "        'avg_energy': avg_energy\n",
    "        \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "model = resnet18()\n",
    "dummy_input = torch.randn(64, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 13:19:30] offline tracker init\n",
      "[codecarbon INFO @ 13:19:30] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 13:19:30] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 13:19:30] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 13:19:30] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 13:19:30] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon WARNING @ 13:19:32] We saw that you have a 12th Gen Intel(R) Core(TM) i9-12900K but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 13:19:32] CPU Model on constant consumption mode: 12th Gen Intel(R) Core(TM) i9-12900K\n",
      "[codecarbon INFO @ 13:19:32] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 13:19:32]   Platform system: Linux-5.15.0-113-generic-x86_64-with-glibc2.31\n",
      "[codecarbon INFO @ 13:19:32]   Python version: 3.9.0\n",
      "[codecarbon INFO @ 13:19:32]   CodeCarbon version: 2.3.4\n",
      "[codecarbon INFO @ 13:19:32]   Available RAM : 125.578 GB\n",
      "[codecarbon INFO @ 13:19:32]   CPU count: 24\n",
      "[codecarbon INFO @ 13:19:32]   CPU model: 12th Gen Intel(R) Core(TM) i9-12900K\n",
      "[codecarbon INFO @ 13:19:32]   GPU count: 1\n",
      "[codecarbon INFO @ 13:19:32]   GPU model: 1 x NVIDIA GeForce RTX 3090\n",
      "[codecarbon INFO @ 13:19:33] Energy consumed for RAM : 0.000016 kWh. RAM Power : 47.091885566711426 W\n",
      "[codecarbon INFO @ 13:19:33] Energy consumed for all GPUs : 0.000109 kWh. Total GPU Power : 328.4733410957834 W\n",
      "[codecarbon INFO @ 13:19:33] Energy consumed for all CPUs : 0.000014 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:19:33] 0.000139 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size: 46.84 MB (disk), 11.690M parameters\n",
      "GPU Latency: 13.110 ms (± 0.022 ms)\n",
      "GPU Throughput: 4881.84 inferences/sec\n",
      "CPU Latency: 475.591 ms (± 6.319 ms)\n",
      "CPU Throughput: 134.57 inferences/sec\n",
      "Model MACs: 116.738G\n",
      "Average GPU Memory Usage: 94.18 MB\n",
      "Peak GPU Memory Usage: 504.97 MB\n",
      "Average Carbon Emissions per Inference: 0.000526 gCO2e\n",
      "Average Energy Consumption per Inference: 0.001386 Wh\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'disk_size': 46835512,\n",
       " 'num_parameters': '11.690M',\n",
       " 'gpu_latency': 13.109815979003907,\n",
       " 'gpu_throughput': 4881.838166340362,\n",
       " 'cpu_latency': 475.5907801212743,\n",
       " 'cpu_throughput': 134.56947164467778,\n",
       " 'macs': '116.738G',\n",
       " 'avg_gpu_memory': 94181376.0,\n",
       " 'peak_gpu_memory': 504967168,\n",
       " 'avg_emissions': 5.256446662000115e-07,\n",
       " 'avg_energy': 1.385974440225733e-06}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|eval: false\n",
    "benchmark(model, dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def evaluate(model, dataloader, device=None, verbose=True):\n",
    "    if device is None: device = torch.device(\"cuda\")\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        local_acc = []\n",
    "        loader = tqdm(dataloader.valid, desc=\"valid\", leave=False)\n",
    "        for i, data in enumerate(loader):\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0) - 1\n",
    "            correct += ((predicted.as_subclass(torch.Tensor) == labels.as_subclass(torch.Tensor)).sum().item())\n",
    "\n",
    "        acc = 100 * correct / total\n",
    "        if verbose:\n",
    "            print(f\"Valid Accuracy: {acc:.2f} %\")\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def compute_model_metrics(model, dls, dummy_input):\n",
    "    metrics = {}\n",
    "    metrics['accuracy'] = round(evaluate(model, dls, device='cpu'), 2)\n",
    "    metrics['latency'] = round(evaluate_cpu_speed(model.to(\"cpu\"), dummy_input=dummy_input)[0] * 1000, 1)\n",
    "    metrics['size'] = get_model_size(model)\n",
    "    try:\n",
    "        metrics['params'] = round(get_num_parameters(model) / 1e6, 2)\n",
    "    except RuntimeError:\n",
    "        metrics['params'] = \"*\"\n",
    "    try:\n",
    "        metrics['mac'] = round(get_model_macs(model, dummy_input) / 1e6)\n",
    "    except (AttributeError, RuntimeError):\n",
    "        metrics['mac'] = \"*\"\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def compare_models(model_list, dls):\n",
    "\n",
    "    metrics_keys = [\"latency\", \"accuracy\", \"params\", \"size\", \"mac\"]\n",
    "    metrics_names = {\n",
    "        \"latency\": \"Latency (ms/sample)\",\n",
    "        \"accuracy\": \"Accuracy (%)\",\n",
    "        \"params\": \"Params (M)\",\n",
    "        \"size\": \"Size (MiB)\",\n",
    "        \"mac\": \"MACs (M)\",\n",
    "    }\n",
    "    table_data = {key: [metrics_names[key]] for key in metrics_keys}\n",
    "    model_names = [\"Original Model\", \"Pruned Model\", \"Quantized Model\"]\n",
    "\n",
    "\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"\"] + model_names\n",
    "    table.align = \"r\"\n",
    "    table.align[\"\"] = \"l\"\n",
    "\n",
    "    dummy_input = next(iter(dls.valid))[0][0][None].to('cpu')\n",
    "\n",
    "    model_metrics_list = []\n",
    "    for model in model_list:\n",
    "        metrics = compute_model_metrics(model, dls, dummy_input)\n",
    "        model_metrics_list.append(metrics)\n",
    "\n",
    "    for metrics in model_metrics_list:\n",
    "        for key in metrics_keys:\n",
    "            table_data[key].append(metrics.get(key, \"*\"))\n",
    "\n",
    "    for key in metrics_keys:\n",
    "        values = table_data[key]\n",
    "        original_value = values[1]\n",
    "        for i in range(2, len(values)):\n",
    "            current_value = values[i]\n",
    "            gain_info = ''\n",
    "            try:\n",
    "                orig_val = float(original_value)\n",
    "                curr_val = float(current_value)\n",
    "                if key == 'accuracy':\n",
    "                    gain = curr_val - orig_val\n",
    "                    gain_info = f'({gain:+.2f}%)'\n",
    "                else:\n",
    "                    gain = orig_val / curr_val if curr_val != 0 else float('inf')\n",
    "                    gain_info = f'({gain:.2f}x)' if gain != float('inf') else '(inf)'\n",
    "            except (ValueError, TypeError):\n",
    "                gain_info = ''\n",
    "            if gain_info:\n",
    "                values[i] = f'{current_value:<8} {gain_info:>8}'\n",
    "            else:\n",
    "                values[i] = f'{current_value}'\n",
    "\n",
    "    for key in metrics_keys:\n",
    "        table.add_row(table_data[key])\n",
    "\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "from fastai.vision.all import *\n",
    "path = untar_data(URLs.PETS)\n",
    "files = get_image_files(path/\"images\")\n",
    "\n",
    "def label_func(f): return f[0].isupper()\n",
    "\n",
    "dls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "learn = vision_learner(dls, resnet18, metrics=accuracy)\n",
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.714393</td>\n",
       "      <td>0.421342</td>\n",
       "      <td>0.841678</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.396920</td>\n",
       "      <td>0.254652</td>\n",
       "      <td>0.888363</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.228655</td>\n",
       "      <td>0.230342</td>\n",
       "      <td>0.907307</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.139857</td>\n",
       "      <td>0.181267</td>\n",
       "      <td>0.933018</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.078330</td>\n",
       "      <td>0.166232</td>\n",
       "      <td>0.935724</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|eval: false\n",
    "learn.fit_one_cycle(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "model = deepcopy(learn.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning until a sparsity of [25]%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.050833</td>\n",
       "      <td>0.203945</td>\n",
       "      <td>0.932341</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.157106</td>\n",
       "      <td>0.257223</td>\n",
       "      <td>0.897835</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.189360</td>\n",
       "      <td>0.263265</td>\n",
       "      <td>0.894452</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity at the end of epoch 0: [5.2]%\n",
      "Sparsity at the end of epoch 1: [24.15]%\n",
      "Sparsity at the end of epoch 2: [25.0]%\n",
      "Final Sparsity: [25.0]%\n"
     ]
    }
   ],
   "source": [
    "#|eval: false\n",
    "from fasterai.prune.all import *\n",
    "\n",
    "pr_cb = PruneCallback(sparsity=25, context='local', criteria=large_final, schedule=one_cycle, layer_type=[nn.Conv2d])\n",
    "learn.fit_one_cycle(3, cbs=pr_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "pruned_model = deepcopy(learn.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/HubensN/miniconda3/envs/clean/lib/python3.9/site-packages/torch/ao/quantization/observer.py:221: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#|eval: false\n",
    "from fasterai.quantize.all import *\n",
    "\n",
    "qt = Quantizer()\n",
    "\n",
    "q_model = qt.quantize(learn.model.to('cpu'), dls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c7e85895dff481c80d3306ecdf84184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Accuracy: 95.12 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a1560d186845938f2fa8cd34e2ac92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Accuracy: 90.92 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e163568b564b3ca2e22af67e71221c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Accuracy: 89.61 %\n",
      "+---------------------+----------------+-------------------+-------------------+\n",
      "|                     | Original Model |      Pruned Model |   Quantized Model |\n",
      "+---------------------+----------------+-------------------+-------------------+\n",
      "| Latency (ms/sample) |         3517.7 | 2480.0    (1.42x) | 2063.2    (1.70x) |\n",
      "| Accuracy (%)        |          95.12 | 90.92    (-4.20%) | 89.61    (-5.51%) |\n",
      "| Params (M)          |           11.7 | 6.69      (1.75x) |                 * |\n",
      "| Size (MiB)          |       46912066 | 26829378  (1.75x) | 6827042   (6.87x) |\n",
      "| MACs (M)            |            149 | 86        (1.73x) |                 * |\n",
      "+---------------------+----------------+-------------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "#|eval: false\n",
    "compare_models([model, pruned_model, q_model], dls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
