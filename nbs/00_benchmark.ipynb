{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# benchmark\n",
    "\n",
    "> A module to benchmark Pytorch model according to: size, speed, compute and energy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from torchprofile import profile_macs\n",
    "from codecarbon import OfflineEmissionsTracker\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_model_size(model, temp_path=\"temp_model.pth\"):\n",
    "    torch.save(model.state_dict(), temp_path)\n",
    "    model_size = os.path.getsize(temp_path)\n",
    "    os.remove(temp_path)\n",
    "    \n",
    "    return model_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_num_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@torch.inference_mode()\n",
    "def evaluate_gpu_speed(model, dummy_input, warmup_rounds=50, test_rounds=100):\n",
    "    device = torch.device(\"cuda\")\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    dummy_input = dummy_input.to(device)\n",
    "    \n",
    "    starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "    latencies = []\n",
    "\n",
    "    # Warm up GPU\n",
    "    for _ in range(warmup_rounds):\n",
    "        _ = model(dummy_input)\n",
    "    \n",
    "    # Measure Latency\n",
    "    for _ in range(test_rounds):\n",
    "        starter.record()\n",
    "        _ = model(dummy_input)\n",
    "        ender.record()\n",
    "        torch.cuda.synchronize()\n",
    "        latencies.append(starter.elapsed_time(ender))  # time in milliseconds\n",
    "    \n",
    "    latencies = np.array(latencies)\n",
    "    mean_latency = np.mean(latencies)\n",
    "    std_latency = np.std(latencies)\n",
    "\n",
    "    # Measure Throughput\n",
    "    throughput = dummy_input.size(0) * 1000 / mean_latency  # Inferences per second\n",
    "\n",
    "    return mean_latency, std_latency, throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@torch.inference_mode()\n",
    "def evaluate_cpu_speed(model, dummy_input, warmup_rounds=50, test_rounds=100):\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    dummy_input = dummy_input.to(device)\n",
    "    \n",
    "    # Warm up CPU\n",
    "    for _ in range(warmup_rounds):\n",
    "        _ = model(dummy_input)\n",
    "    \n",
    "    # Measure Latency\n",
    "    latencies = []\n",
    "    for _ in range(test_rounds):\n",
    "        start_time = time.perf_counter()\n",
    "        _ = model(dummy_input)\n",
    "        end_time = time.perf_counter()\n",
    "        latencies.append(end_time - start_time)\n",
    "    \n",
    "    latencies = np.array(latencies) * 1000  # Convert to milliseconds\n",
    "    mean_latency = np.mean(latencies)\n",
    "    std_latency = np.std(latencies)\n",
    "\n",
    "    # Measure Throughput\n",
    "    throughput = dummy_input.size(0) * 1000 / mean_latency  # Inferences per second\n",
    "\n",
    "    return mean_latency, std_latency, throughput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@torch.inference_mode()\n",
    "def get_model_macs(model, inputs) -> int:\n",
    "    return profile_macs(model, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@torch.inference_mode()\n",
    "def evaluate_gpu_memory_usage(model, dummy_input, warmup_rounds=10, test_rounds=100):\n",
    "    device = torch.device(\"cuda\")\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    dummy_input = dummy_input.to(device)\n",
    "    \n",
    "    # Warm up GPU\n",
    "    for _ in range(warmup_rounds):\n",
    "        _ = model(dummy_input)\n",
    "    \n",
    "    # Measure Memory Usage\n",
    "    memory_usages = []\n",
    "    for _ in range(test_rounds):\n",
    "        torch.cuda.reset_peak_memory_stats(device)\n",
    "        _ = model(dummy_input)\n",
    "        torch.cuda.synchronize()\n",
    "        memory_usages.append(torch.cuda.memory_allocated(device))\n",
    "    \n",
    "    memory_usages = np.array(memory_usages)\n",
    "    average_memory_usage = np.mean(memory_usages)\n",
    "    peak_memory_usage = torch.cuda.max_memory_allocated(device)\n",
    "    \n",
    "    return average_memory_usage, peak_memory_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def evaluate_emissions(model, dummy_input, warmup_rounds=50, test_rounds=100):\n",
    "    device = torch.device(\"cuda\")\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    dummy_input = dummy_input.to(device)\n",
    "\n",
    "    # Warm up GPU\n",
    "    for _ in range(warmup_rounds):\n",
    "        _ = model(dummy_input)\n",
    "    \n",
    "    # Measure Latency\n",
    "    tracker = OfflineEmissionsTracker(country_iso_code=\"USA\")\n",
    "    tracker.start()\n",
    "    for _ in range(test_rounds):\n",
    "        _ = model(dummy_input)\n",
    "    tracker.stop()\n",
    "    total_emissions = tracker.final_emissions\n",
    "    total_energy_consumed = tracker.final_emissions_data.energy_consumed\n",
    "    \n",
    "    # Calculate average emissions and energy consumption per inference\n",
    "    average_emissions_per_inference = total_emissions / test_rounds\n",
    "    average_energy_per_inference = total_energy_consumed / test_rounds\n",
    "    \n",
    "    return average_emissions_per_inference, average_energy_per_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def format_number(num):\n",
    "    if num >= 1e9:\n",
    "        return f\"{num/1e9:.2f} B\"\n",
    "    elif num >= 1e6:\n",
    "        return f\"{num/1e6:.2f} M\"\n",
    "    elif num >= 1e3:\n",
    "        return f\"{num/1e3:.2f} K\"\n",
    "    else:\n",
    "        return f\"{num:.2f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@torch.inference_mode()\n",
    "def benchmark(model, dummy_input):\n",
    "    # Model Size\n",
    "    disk_size = get_model_size(model)\n",
    "    num_parameters = get_num_parameters(model)\n",
    "    \n",
    "    # GPU Speed\n",
    "    gpu_latency, gpu_std_latency, gpu_throughput = evaluate_gpu_speed(model, dummy_input)\n",
    "    \n",
    "    # CPU Speed\n",
    "    cpu_latency, cpu_std_latency, cpu_throughput = evaluate_cpu_speed(model, dummy_input)\n",
    "    \n",
    "    # Model MACs\n",
    "    macs = get_model_macs(model, dummy_input)\n",
    "    \n",
    "    # GPU Memory Usage\n",
    "    avg_gpu_memory, peak_gpu_memory = evaluate_gpu_memory_usage(model, dummy_input)\n",
    "    \n",
    "    # Emissions\n",
    "    avg_emissions, avg_energy = evaluate_emissions(model, dummy_input)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {format_number(num_parameters)} parameters\")\n",
    "    print(f\"GPU Latency: {gpu_latency:.3f} ms (± {gpu_std_latency:.3f} ms)\")\n",
    "    print(f\"GPU Throughput: {gpu_throughput:.2f} inferences/sec\")\n",
    "    print(f\"CPU Latency: {cpu_latency:.3f} ms (± {cpu_std_latency:.3f} ms)\")\n",
    "    print(f\"CPU Throughput: {cpu_throughput:.2f} inferences/sec\")\n",
    "    print(f\"Model MACs: {format_number(macs)}\")\n",
    "    print(f\"Average GPU Memory Usage: {avg_gpu_memory / 1e6:.2f} MB\")\n",
    "    print(f\"Peak GPU Memory Usage: {peak_gpu_memory / 1e6:.2f} MB\")\n",
    "    print(f\"Average Carbon Emissions per Inference: {avg_emissions*1e3:.6f} gCO2e\")\n",
    "    print(f\"Average Energy Consumption per Inference: {avg_energy*1e3:.6f} Wh\")\n",
    "\n",
    "    return {\n",
    "\n",
    "        'disk_size': disk_size,\n",
    "        'num_parameters': num_parameters, \n",
    "        'gpu_latency': gpu_latency, \n",
    "        'gpu_throughput': gpu_throughput,\n",
    "        'cpu_latency': cpu_latency,\n",
    "        'cpu_throughput': cpu_throughput,\n",
    "        'macs': macs, \n",
    "        'avg_gpu_memory': avg_gpu_memory, \n",
    "        'peak_gpu_memory': peak_gpu_memory,\n",
    "        'avg_emissions': avg_emissions, \n",
    "        'avg_energy': avg_energy\n",
    "        \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "\n",
    "model = resnet18()\n",
    "dummy_input = torch.randn(64, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 13:54:16] offline tracker init\n",
      "[codecarbon INFO @ 13:54:16] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 13:54:16] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 13:54:16] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 13:54:16] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 13:54:16] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon WARNING @ 13:54:17] We saw that you have a 12th Gen Intel(R) Core(TM) i9-12900K but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 13:54:17] CPU Model on constant consumption mode: 12th Gen Intel(R) Core(TM) i9-12900K\n",
      "[codecarbon INFO @ 13:54:17] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 13:54:17]   Platform system: Linux-5.15.0-113-generic-x86_64-with-glibc2.31\n",
      "[codecarbon INFO @ 13:54:17]   Python version: 3.9.0\n",
      "[codecarbon INFO @ 13:54:17]   CodeCarbon version: 2.3.4\n",
      "[codecarbon INFO @ 13:54:17]   Available RAM : 125.578 GB\n",
      "[codecarbon INFO @ 13:54:17]   CPU count: 24\n",
      "[codecarbon INFO @ 13:54:17]   CPU model: 12th Gen Intel(R) Core(TM) i9-12900K\n",
      "[codecarbon INFO @ 13:54:17]   GPU count: 1\n",
      "[codecarbon INFO @ 13:54:17]   GPU model: 1 x NVIDIA GeForce RTX 3090\n",
      "[codecarbon INFO @ 13:54:19] Energy consumed for RAM : 0.000016 kWh. RAM Power : 47.091885566711426 W\n",
      "[codecarbon INFO @ 13:54:19] Energy consumed for all GPUs : 0.000109 kWh. Total GPU Power : 328.8707845432614 W\n",
      "[codecarbon INFO @ 13:54:19] Energy consumed for all CPUs : 0.000014 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 13:54:19] 0.000139 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size: 46.84 MB (disk), 11.69 M parameters\n",
      "GPU Latency: 13.116 ms (± 0.038 ms)\n",
      "GPU Throughput: 4879.53 inferences/sec\n",
      "CPU Latency: 502.583 ms (± 10.011 ms)\n",
      "CPU Throughput: 127.34 inferences/sec\n",
      "Model MACs: 116.26 B\n",
      "Average GPU Memory Usage: 227.64 MB\n",
      "Peak GPU Memory Usage: 638.43 MB\n",
      "Average Carbon Emissions per Inference: 0.000528 gCO2e\n",
      "Average Energy Consumption per Inference: 0.001392 Wh\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'disk_size': 46835512,\n",
       " 'num_parameters': 11689512,\n",
       " 'gpu_latency': 13.116016960144043,\n",
       " 'gpu_throughput': 4879.530134375272,\n",
       " 'cpu_latency': 502.5826620403677,\n",
       " 'cpu_throughput': 127.34223608147367,\n",
       " 'macs': 116259684352,\n",
       " 'avg_gpu_memory': 227642368.0,\n",
       " 'peak_gpu_memory': 638428160,\n",
       " 'avg_emissions': 5.278229550241391e-07,\n",
       " 'avg_energy': 1.391717963993406e-06}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark(model, dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
