{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# benchmark\n",
    "\n",
    "> A module to benchmark Pytorch model according to: size, speed, compute and energy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import time\n",
    "from codecarbon import OfflineEmissionsTracker\n",
    "import numpy as np\n",
    "import os\n",
    "from thop import profile, clever_format\n",
    "from tqdm.notebook import tqdm\n",
    "from prettytable import PrettyTable\n",
    "from torchprofile import profile_macs\n",
    "import torch.nn as nn\n",
    "from typing import Dict, List, Tuple, Any, Union, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_model_size(model: nn.Module,            # The model we want to study\n",
    "                   temp_path: str =\"temp.pth\"   # The temporary path used to save the model\n",
    "                  ) -> int:\n",
    "    try: model.save(temp_path)\n",
    "    except: torch.save(model.state_dict(), temp_path)\n",
    "    \n",
    "    model_size = os.path.getsize(temp_path)\n",
    "    os.remove(temp_path)\n",
    "    \n",
    "    return model_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_num_parameters(model: nn.Module  # The model we want to study\n",
    "                      ) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@torch.inference_mode()\n",
    "def evaluate_gpu_speed(model:nn.Module,            # The model we want to evaluate\n",
    "                       dummy_input: torch.Tensor,  # The input used to evaluate the model\n",
    "                       warmup_rounds:int=50,       # The amount of warmup iterations\n",
    "                       test_rounds:int=100        # The amount of iterations used to evaluate the speed\n",
    "                      )-> Tuple[float, float, float]:\n",
    "    device = torch.device(\"cuda\")\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    dummy_input = dummy_input.to(device)\n",
    "    \n",
    "    starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "    latencies = []\n",
    "\n",
    "    # Warm up GPU\n",
    "    for _ in range(warmup_rounds):\n",
    "        _ = model(dummy_input)\n",
    "    \n",
    "    # Measure Latency\n",
    "    for _ in range(test_rounds):\n",
    "        starter.record()\n",
    "        _ = model(dummy_input)\n",
    "        ender.record()\n",
    "        torch.cuda.synchronize()\n",
    "        latencies.append(starter.elapsed_time(ender))  # time in milliseconds\n",
    "    \n",
    "    latencies = np.array(latencies)\n",
    "    mean_latency = np.mean(latencies)\n",
    "    std_latency = np.std(latencies)\n",
    "\n",
    "    # Measure Throughput\n",
    "    throughput = dummy_input.size(0) * 1000 / mean_latency  # Inferences per second\n",
    "\n",
    "    return mean_latency, std_latency, throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@torch.inference_mode()\n",
    "def evaluate_cpu_speed(model:nn.Module,            # The model we want to evaluate\n",
    "                       dummy_input: torch.Tensor,  # The input used to evaluate the model\n",
    "                       warmup_rounds:int=50,       # The amount of warmup iterations\n",
    "                       test_rounds:int=100        # The amount of iterations used to evaluate the speed\n",
    "                      )-> Tuple[float, float, float]:\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    dummy_input = dummy_input.to(device)\n",
    "    \n",
    "    # Warm up CPU\n",
    "    for _ in range(warmup_rounds):\n",
    "        _ = model(dummy_input)\n",
    "    \n",
    "    # Measure Latency\n",
    "    latencies = []\n",
    "    for _ in range(test_rounds):\n",
    "        start_time = time.perf_counter()\n",
    "        _ = model(dummy_input)\n",
    "        end_time = time.perf_counter()\n",
    "        latencies.append(end_time - start_time)\n",
    "    \n",
    "    latencies = np.array(latencies) * 1000  # Convert to milliseconds\n",
    "    mean_latency = np.mean(latencies)\n",
    "    std_latency = np.std(latencies)\n",
    "\n",
    "    # Measure Throughput\n",
    "    throughput = dummy_input.size(0) * 1000 / mean_latency  # Inferences per second\n",
    "\n",
    "    return mean_latency, std_latency, throughput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@torch.inference_mode()\n",
    "def get_model_macs(model,  # The model we want to evaluate\n",
    "                   inputs  # The input used to evaluate\n",
    "                  ) -> int:\n",
    "    return profile_macs(model, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@torch.inference_mode()\n",
    "def evaluate_gpu_memory_usage(model:nn.Module,             # The model we want to evaluate\n",
    "                              dummy_input:torch.Tensor,       # The input used to evaluate\n",
    "                              warmup_rounds:int=10,  # The amount of warmup iterations\n",
    "                              test_rounds:int=100    # The amount of iterations used to evaluate the speed\n",
    "                             )-> Tuple[float, float]:\n",
    "    device = torch.device(\"cuda\")\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    dummy_input = dummy_input.to(device)\n",
    "    \n",
    "    # Warm up GPU\n",
    "    for _ in range(warmup_rounds):\n",
    "        _ = model(dummy_input)\n",
    "    \n",
    "    # Measure Memory Usage\n",
    "    memory_usages = []\n",
    "    for _ in range(test_rounds):\n",
    "        torch.cuda.reset_peak_memory_stats(device)\n",
    "        _ = model(dummy_input)\n",
    "        torch.cuda.synchronize()\n",
    "        memory_usages.append(torch.cuda.memory_allocated(device))\n",
    "    \n",
    "    memory_usages = np.array(memory_usages)\n",
    "    average_memory_usage = np.mean(memory_usages)\n",
    "    peak_memory_usage = torch.cuda.max_memory_allocated(device)\n",
    "    \n",
    "    return average_memory_usage, peak_memory_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@torch.inference_mode()\n",
    "def evaluate_emissions(model:nn.Module,            # The model we want to evaluate\n",
    "                       dummy_input: torch.Tensor,  # The input used to evaluate the model\n",
    "                       warmup_rounds:int=50,       # The amount of warmup iterations\n",
    "                       test_rounds:int=100        # The amount of iterations used to evaluate the speed\n",
    "                      )-> Tuple[float, float]:\n",
    "    device = torch.device(\"cuda\")\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    dummy_input = dummy_input.to(device)\n",
    "\n",
    "    # Warm up GPU\n",
    "    for _ in range(warmup_rounds):\n",
    "        _ = model(dummy_input)\n",
    "    \n",
    "    # Measure Latency\n",
    "    tracker = OfflineEmissionsTracker(country_iso_code=\"USA\")\n",
    "    tracker.start()\n",
    "    for _ in range(test_rounds):\n",
    "        _ = model(dummy_input)\n",
    "    tracker.stop()\n",
    "    total_emissions = tracker.final_emissions\n",
    "    total_energy_consumed = tracker.final_emissions_data.energy_consumed\n",
    "    \n",
    "    # Calculate average emissions and energy consumption per inference\n",
    "    average_emissions_per_inference = total_emissions / test_rounds\n",
    "    average_energy_per_inference = total_energy_consumed / test_rounds\n",
    "    \n",
    "    return average_emissions_per_inference, average_energy_per_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@torch.inference_mode()\n",
    "def benchmark(model:nn.Module,          # The model we want to benchmark\n",
    "              dummy_input:torch.Tensor  # The input used to benchmark the model\n",
    "             )-> Dict[str, Union[float, str]]:\n",
    "    # Model Size\n",
    "    disk_size = get_model_size(model)\n",
    "    #num_parameters = get_num_parameters(model)\n",
    "    \n",
    "    # GPU Speed\n",
    "    gpu_latency, gpu_std_latency, gpu_throughput = evaluate_gpu_speed(model, dummy_input)\n",
    "    \n",
    "    # CPU Speed\n",
    "    cpu_latency, cpu_std_latency, cpu_throughput = evaluate_cpu_speed(model, dummy_input)\n",
    "    \n",
    "    # Model MACs\n",
    "    #macs = get_model_macs(model, dummy_input)\n",
    "    macs, params = profile(model, inputs=(dummy_input, ))\n",
    "    macs, num_parameters = clever_format([macs, params], \"%.3f\")\n",
    "    \n",
    "    # GPU Memory Usage\n",
    "    avg_gpu_memory, peak_gpu_memory = evaluate_gpu_memory_usage(model, dummy_input)\n",
    "    \n",
    "    # Emissions\n",
    "    avg_emissions, avg_energy = evaluate_emissions(model, dummy_input)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters} parameters\")\n",
    "    print(f\"GPU Latency: {gpu_latency:.3f} ms (± {gpu_std_latency:.3f} ms)\")\n",
    "    print(f\"GPU Throughput: {gpu_throughput:.2f} inferences/sec\")\n",
    "    print(f\"CPU Latency: {cpu_latency:.3f} ms (± {cpu_std_latency:.3f} ms)\")\n",
    "    print(f\"CPU Throughput: {cpu_throughput:.2f} inferences/sec\")\n",
    "    print(f\"Model MACs: {macs}\")\n",
    "    print(f\"Average GPU Memory Usage: {avg_gpu_memory / 1e6:.2f} MB\")\n",
    "    print(f\"Peak GPU Memory Usage: {peak_gpu_memory / 1e6:.2f} MB\")\n",
    "    print(f\"Average Carbon Emissions per Inference: {avg_emissions*1e3:.6f} gCO2e\")\n",
    "    print(f\"Average Energy Consumption per Inference: {avg_energy*1e3:.6f} Wh\")\n",
    "\n",
    "    return {\n",
    "\n",
    "        'disk_size': disk_size,\n",
    "        'num_parameters': num_parameters, \n",
    "        'gpu_latency': gpu_latency, \n",
    "        'gpu_throughput': gpu_throughput,\n",
    "        'cpu_latency': cpu_latency,\n",
    "        'cpu_throughput': cpu_throughput,\n",
    "        'macs': macs, \n",
    "        'avg_gpu_memory': avg_gpu_memory, \n",
    "        'peak_gpu_memory': peak_gpu_memory,\n",
    "        'avg_emissions': avg_emissions, \n",
    "        'avg_energy': avg_energy\n",
    "        \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/FasterAI-Labs/fasterbench/blob/main/fasterbench/benchmark.py#L179){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### benchmark\n",
       "\n",
       ">      benchmark (model:torch.nn.modules.module.Module,\n",
       ">                 dummy_input:torch.Tensor)\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| model | Module | The model we want to benchmark |\n",
       "| dummy_input | Tensor | The input used to benchmark the model |\n",
       "| **Returns** | **Dict** |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/FasterAI-Labs/fasterbench/blob/main/fasterbench/benchmark.py#L179){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### benchmark\n",
       "\n",
       ">      benchmark (model:torch.nn.modules.module.Module,\n",
       ">                 dummy_input:torch.Tensor)\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| model | Module | The model we want to benchmark |\n",
       "| dummy_input | Tensor | The input used to benchmark the model |\n",
       "| **Returns** | **Dict** |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@torch.inference_mode()\n",
    "def evaluate(model:nn.Module,   # The module to evaluate\n",
    "             dls:Any,           # The dataloader used to evaluate \n",
    "             device=None,       # The device used to evaluate\n",
    "             verbose:bool=False # Add some verbose\n",
    "            )->int:\n",
    "    if device is None: device = torch.device(\"cuda\")\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        local_acc = []\n",
    "        loader = tqdm(dls.valid, desc=\"valid\", leave=False)\n",
    "        for i, data in enumerate(loader):\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0) - 1\n",
    "            correct += ((predicted.as_subclass(torch.Tensor) == labels.as_subclass(torch.Tensor)).sum().item())\n",
    "\n",
    "        acc = 100 * correct / total\n",
    "        if verbose:\n",
    "            print(f\"Valid Accuracy: {acc:.2f} %\")\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def compute_model_metrics(model:nn.Module,          # The model we want to evaluate\n",
    "                          dls:Any,                  # The dataloader used to measure accuracy\n",
    "                          dummy_input:torch.Tensor # The input used to evaluate the model\n",
    "                         ) -> Dict[str, Union[float, str]]:\n",
    "    metrics = {}\n",
    "    metrics['accuracy'] = round(evaluate(model, dls, device='cpu'), 2)\n",
    "    metrics['latency'] = round(evaluate_cpu_speed(model.to(\"cpu\"), dummy_input=dummy_input)[0] * 1000, 1)\n",
    "    metrics['size'] = get_model_size(model)\n",
    "    try:\n",
    "        metrics['params'] = round(get_num_parameters(model) / 1e6, 2)\n",
    "    except RuntimeError:\n",
    "        metrics['params'] = \"*\"\n",
    "    try:\n",
    "        metrics['mac'] = round(get_model_macs(model, dummy_input) / 1e6)\n",
    "    except (AttributeError, RuntimeError):\n",
    "        metrics['mac'] = \"*\"\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@torch.inference_mode()\n",
    "def compare_models(model_list:List[nn.Module], # The list of models we want to compare\n",
    "                   dls: Any                    # The dataloader used to compare models\n",
    "                  )-> None:\n",
    "\n",
    "    metrics_keys = [\"latency\", \"accuracy\", \"params\", \"size\", \"mac\"]\n",
    "    metrics_names = {\n",
    "        \"latency\": \"Latency (ms/sample)\",\n",
    "        \"accuracy\": \"Accuracy (%)\",\n",
    "        \"params\": \"Params (M)\",\n",
    "        \"size\": \"Size (MiB)\",\n",
    "        \"mac\": \"MACs (M)\",\n",
    "    }\n",
    "    table_data = {key: [metrics_names[key]] for key in metrics_keys}\n",
    "    model_names = [\"Original Model\", \"Pruned Model\", \"Quantized Model\"]\n",
    "\n",
    "\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"\"] + model_names\n",
    "    table.align = \"r\"\n",
    "    table.align[\"\"] = \"l\"\n",
    "    table.align[\"Original Model\"] = \"l\"\n",
    "\n",
    "    dummy_input = torch.Tensor(next(iter(dls.valid))[0][0][None]).to('cpu')\n",
    "\n",
    "    model_metrics_list = []\n",
    "    for model in model_list:\n",
    "        metrics = compute_model_metrics(model, dls, dummy_input)\n",
    "        model_metrics_list.append(metrics)\n",
    "\n",
    "    for metrics in model_metrics_list:\n",
    "        for key in metrics_keys:\n",
    "            table_data[key].append(metrics.get(key, \"*\"))\n",
    "\n",
    "    for key in metrics_keys:\n",
    "        values = table_data[key]\n",
    "        original_value = values[1]\n",
    "        for i in range(2, len(values)):\n",
    "            current_value = values[i]\n",
    "            gain_info = ''\n",
    "            try:\n",
    "                orig_val = float(original_value)\n",
    "                curr_val = float(current_value)\n",
    "                if key == 'accuracy':\n",
    "                    gain = curr_val - orig_val\n",
    "                    gain_info = f'({gain:+.2f}%)'\n",
    "                else:\n",
    "                    gain = orig_val / curr_val if curr_val != 0 else float('inf')\n",
    "                    gain_info = f'({gain:.2f}x)' if gain != float('inf') else '(inf)'\n",
    "            except (ValueError, TypeError):\n",
    "                gain_info = ''\n",
    "            if gain_info:\n",
    "                values[i] = f'{current_value:<8} {gain_info:>8}'\n",
    "            else:\n",
    "                values[i] = f'{current_value}'\n",
    "\n",
    "    for key in metrics_keys:\n",
    "        table.add_row(table_data[key])\n",
    "\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/FasterAI-Labs/fasterbench/blob/main/fasterbench/benchmark.py#L281){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### compare_models\n",
       "\n",
       ">      compare_models (model_list:List[torch.nn.modules.module.Module], dls:Any)\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| model_list | List | The list of models we want to compare |\n",
       "| dls | Any | The dataloader used to compare models |\n",
       "| **Returns** | **None** |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/FasterAI-Labs/fasterbench/blob/main/fasterbench/benchmark.py#L281){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### compare_models\n",
       "\n",
       ">      compare_models (model_list:List[torch.nn.modules.module.Module], dls:Any)\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| model_list | List | The list of models we want to compare |\n",
       "| dls | Any | The dataloader used to compare models |\n",
       "| **Returns** | **None** |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(compare_models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
