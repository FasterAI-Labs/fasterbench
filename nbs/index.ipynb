{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fasterbench\n",
    "\n",
    "> Comprehensive benchmarking toolkit for deep learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![PyPI version](https://badge.fury.io/py/fasterbench.svg)](https://badge.fury.io/py/fasterbench)\n",
    "[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n",
    "[![CI](https://github.com/FasterAI-Labs/fasterbench/actions/workflows/test.yaml/badge.svg)](https://github.com/FasterAI-Labs/fasterbench/actions/workflows/test.yaml)\n",
    "\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "`fasterbench` is a comprehensive benchmarking library for PyTorch models that helps AI researchers and engineers evaluate model performance across five critical dimensions:\n",
    "\n",
    "- **Size**: Model disk size and parameter count\n",
    "- **Speed**: Latency and throughput on both GPU and CPU\n",
    "- **Compute**: MACs (multiply-accumulate operations)\n",
    "- **Memory**: Peak and average memory consumption\n",
    "- **Energy**: Power consumption and carbon emissions\n",
    "\n",
    "Whether you're optimizing for edge deployment, comparing model architectures, or researching model efficiency, FasterBench provides the metrics you need with minimal setup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "```bash\n",
    "pip install fasterbench\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import torch\n",
    "from torchvision.models import resnet18\n",
    "from fasterbench import benchmark\n",
    "\n",
    "# Load your model\n",
    "model = resnet18()\n",
    "\n",
    "# Create sample input\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "# Run comprehensive benchmarks\n",
    "results = benchmark(model, dummy_input)\n",
    "\n",
    "# Print results\n",
    "for metric, value in results.items():\n",
    "    print(f\"{metric}: {value}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "### All-in-one Benchmarking\n",
    "\n",
    "Get comprehensive metrics with a single function call:\n",
    "\n",
    "```python\n",
    "# Measure all metrics\n",
    "results = benchmark(model, dummy_input)\n",
    "\n",
    "# Or select specific metrics\n",
    "results = benchmark(model, dummy_input, metrics=[\"size\", \"speed\"])\n",
    "```\n",
    "\n",
    "### Size Metrics\n",
    "\n",
    "Evaluate model size characteristics:\n",
    "\n",
    "```python\n",
    "from fasterbench import compute_size\n",
    "\n",
    "size_metrics = compute_size(model)\n",
    "print(f\"Disk Size: {size_metrics.size_mib:.2f} MiB\")\n",
    "print(f\"Parameters: {size_metrics.num_params:,}\")\n",
    "```\n",
    "\n",
    "### Speed Metrics\n",
    "\n",
    "Measure inference performance across devices:\n",
    "\n",
    "```python\n",
    "from fasterbench import compute_speed_multi\n",
    "\n",
    "speed_metrics = compute_speed_multi(model, dummy_input)\n",
    "for device, metrics in speed_metrics.items():\n",
    "    print(f\"{device} latency (P50): {metrics.p50_ms:.2f} ms\")\n",
    "    print(f\"{device} throughput: {metrics.throughput_s:.2f} inferences/sec\")\n",
    "```\n",
    "\n",
    "### Compute Metrics\n",
    "\n",
    "Quantify computational complexity:\n",
    "\n",
    "```python\n",
    "from fasterbench import compute_compute\n",
    "\n",
    "compute_metrics = compute_compute(model, dummy_input)\n",
    "print(f\"MACs: {compute_metrics.macs_m} million\")\n",
    "```\n",
    "\n",
    "### Memory Metrics\n",
    "\n",
    "Profile memory usage:\n",
    "\n",
    "```python\n",
    "from fasterbench import compute_memory_multi\n",
    "\n",
    "memory_metrics = compute_memory_multi(model, dummy_input)\n",
    "for device, metrics in memory_metrics.items():\n",
    "    print(f\"{device} peak memory: {metrics.peak_mib:.2f} MiB\")\n",
    "```\n",
    "\n",
    "### Energy Metrics\n",
    "\n",
    "Measure environmental impact:\n",
    "\n",
    "```python\n",
    "from fasterbench import compute_energy_multi\n",
    "\n",
    "# Requires codecarbon package\n",
    "energy_metrics = compute_energy_multi(model, dummy_input)\n",
    "for device, metrics in energy_metrics.items():\n",
    "    print(f\"{device} power usage: {metrics.mean_watts:.2f} W\")\n",
    "    print(f\"{device} CO2: {metrics.co2_eq_g:.6f} g COâ‚‚-eq per inference\")\n",
    "```\n",
    "\n",
    "### Thread Count Optimization\n",
    "\n",
    "Find the optimal number of CPU threads:\n",
    "\n",
    "```python\n",
    "from fasterbench import sweep_threads\n",
    "\n",
    "thread_results = sweep_threads(model, dummy_input, thread_counts=[1, 2, 4, 8, 16])\n",
    "for result in thread_results:\n",
    "    print(f\"Threads: {result['threads']}, Latency: {result['mean_ms']:.2f} ms\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Results\n",
    "\n",
    "Create radar plots to compare multiple models:\n",
    "\n",
    "```python\n",
    "from fasterbench.benchmark import benchmark\n",
    "from fasterbench.plot import *\n",
    "from torchvision.models import resnet18, mobilenet_v3_large\n",
    "import torch\n",
    "\n",
    "dummy = torch.randn(8,3,224,224)\n",
    "\n",
    "resnet   = benchmark(resnet18(),            dummy,\n",
    "                     metrics=(\"size\",\"speed\",\"compute\",\"energy\"))\n",
    "mobilenet= benchmark(mobilenet_v3_large(),  dummy,\n",
    "                     metrics=(\"size\",\"speed\",\"compute\",\"energy\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "fig = create_radar_plot([resnet, mobilenet],\n",
    "                        model_names=[\"ResNet-18\", \"MobileNet-V3\"])\n",
    "fig.show()\n",
    "```\n",
    "\n",
    "![](nbs/imgs/newplot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "For more detailed usage examples and API documentation, visit our [documentation](https://github.com/nathanhubens/fasterbench).\n",
    "\n",
    "## Contributing\n",
    "\n",
    "Contributions are welcome! Please feel free to submit a Pull Request.\n",
    "\n",
    "## License\n",
    "\n",
    "This project is licensed under the Apache 2.0 License - see the LICENSE file for details."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
