{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory\n",
    "\n",
    "> Memory modules for benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "\n",
    "import warnings\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, Iterable, List\n",
    "from fasterbench.core import _bytes_to_mib\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    import psutil\n",
    "except ImportError:\n",
    "    psutil = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass(slots=True)\n",
    "class MemoryMetrics:\n",
    "    \"\"\"Average & peak resident memory (MiB) for one device.\"\"\"\n",
    "\n",
    "    avg_mib: float\n",
    "    peak_mib: float\n",
    "    reserved_mib: float  # GPU‑only (NaN for CPU)\n",
    "    device: str\n",
    "\n",
    "    def as_dict(self) -> Dict[str, float | str]:\n",
    "        return asdict(self)\n",
    "\n",
    "\n",
    "#| export\n",
    "def _gpu_metrics(\n",
    "    model: torch.nn.Module,\n",
    "    dummy_input: torch.Tensor,\n",
    "    *,\n",
    "    warmup_rounds: int,\n",
    "    test_rounds: int,\n",
    ") -> MemoryMetrics:\n",
    "    dev = torch.device(\"cuda\")\n",
    "    model = model.eval().to(dev)\n",
    "    dummy_input = dummy_input.to(dev)\n",
    "\n",
    "    # warm‑up\n",
    "    for _ in range(warmup_rounds):\n",
    "        model(dummy_input)\n",
    "    torch.cuda.synchronize(dev)\n",
    "\n",
    "    alloc, alloc_peak, reserv_peak = [], [], []\n",
    "    for _ in range(test_rounds):\n",
    "        torch.cuda.reset_peak_memory_stats(dev)\n",
    "        model(dummy_input)\n",
    "        torch.cuda.synchronize(dev)\n",
    "        alloc.append(torch.cuda.memory_allocated(dev))\n",
    "        alloc_peak.append(torch.cuda.max_memory_allocated(dev))\n",
    "        reserv_peak.append(torch.cuda.max_memory_reserved(dev))\n",
    "\n",
    "    return MemoryMetrics(\n",
    "        avg_mib=_bytes_to_mib(float(np.mean(alloc))),\n",
    "        peak_mib=_bytes_to_mib(float(np.mean(alloc_peak))),\n",
    "        reserved_mib=_bytes_to_mib(float(np.mean(reserv_peak))),\n",
    "        device=\"cuda\",\n",
    "    )\n",
    "\n",
    "#| export\n",
    "def _cpu_metrics(\n",
    "    model: torch.nn.Module,\n",
    "    dummy_input: torch.Tensor,\n",
    "    *,\n",
    "    warmup_rounds: int,\n",
    "    test_rounds: int,\n",
    ") -> MemoryMetrics:\n",
    "    if psutil is None:\n",
    "        warnings.warn(\"psutil not available – returning NaNs for CPU memory metrics\")\n",
    "        nan = float(\"nan\")\n",
    "        return MemoryMetrics(nan, nan, nan, \"cpu\")\n",
    "\n",
    "    proc = psutil.Process()\n",
    "    model = model.eval().cpu()\n",
    "    dummy_input = dummy_input.cpu()\n",
    "\n",
    "    # establish baseline RSS before any warm‑up\n",
    "    rss0 = proc.memory_info().rss\n",
    "\n",
    "    for _ in range(warmup_rounds):\n",
    "        model(dummy_input)\n",
    "\n",
    "    diffs: List[int] = []\n",
    "    peaks: List[int] = []\n",
    "    for _ in range(test_rounds):\n",
    "        rss_before = proc.memory_info().rss\n",
    "        model(dummy_input)\n",
    "        rss_after = proc.memory_info().rss\n",
    "        diffs.append(max(0, rss_after - rss_before))\n",
    "        peaks.append(max(0, rss_after - rss0))\n",
    "\n",
    "    return MemoryMetrics(\n",
    "        avg_mib=_bytes_to_mib(float(np.mean(diffs))),\n",
    "        peak_mib=_bytes_to_mib(float(np.max(peaks))),\n",
    "        reserved_mib=float(\"nan\"),\n",
    "        device=\"cpu\",\n",
    "    )\n",
    "\n",
    "\n",
    "#| export\n",
    "def compute_memory(\n",
    "    model: torch.nn.Module,\n",
    "    dummy_input: torch.Tensor,\n",
    "    *,\n",
    "    warmup_rounds: int = 10,\n",
    "    test_rounds: int = 100,\n",
    ") -> MemoryMetrics:\n",
    "    \"\"\"Alias for GPU if available, else CPU.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return _gpu_metrics(model, dummy_input, warmup_rounds=warmup_rounds, test_rounds=test_rounds)\n",
    "    return _cpu_metrics(model, dummy_input, warmup_rounds=warmup_rounds, test_rounds=test_rounds)\n",
    "\n",
    "#| export\n",
    "def compute_memory_multi(\n",
    "    model: torch.nn.Module,\n",
    "    dummy_input: torch.Tensor,\n",
    "    *,\n",
    "    devices: Iterable[str] | None = None,\n",
    "    warmup_rounds: int = 10,\n",
    "    test_rounds: int = 100,\n",
    ") -> Dict[str, MemoryMetrics]:\n",
    "    \"\"\"Return memory metrics for each device in *devices* (default cpu + cuda).\"\"\"\n",
    "\n",
    "    if devices is None:\n",
    "        devices = [\"cpu\"]\n",
    "        if torch.cuda.is_available():\n",
    "            devices.append(\"cuda\")\n",
    "\n",
    "    metrics: Dict[str, MemoryMetrics] = {}\n",
    "    for d in devices:\n",
    "        if d == \"cuda\" and not torch.cuda.is_available():\n",
    "            continue\n",
    "        if d == \"cuda\":\n",
    "            metrics[d] = _gpu_metrics(\n",
    "                model, dummy_input, warmup_rounds=warmup_rounds, test_rounds=test_rounds\n",
    "            )\n",
    "        else:\n",
    "            metrics[d] = _cpu_metrics(\n",
    "                model, dummy_input, warmup_rounds=warmup_rounds, test_rounds=test_rounds\n",
    "            )\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
